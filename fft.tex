Psychological scientists have developed FFTs (Fast and Frugal Trees) as one way
  to generate comprehensible models consisting of
   separate tiny rules~\cite{phillips2017fftrees,chen2018applications,martignon2008categorization}.
      A FFT is a decision tree made for binary classification problem with exactly two branches extending
        from each node, where either one or both branches is an exit
        branch leading to a leaf~\cite{martignon2008categorization}. 
        That is to say, in an FFT,  every question posed by a node will
        trigger an immediate decision
        (so humans can read every leaf node
        as a separate rule).



We used the similar implementation of FFT as offered by Fu and Chen et al.~\cite{fu2018building,chen2018applications}. 
An FFT of depth $d$ has a choice of two ``exit policies'' at each level: the existing branch can select for the negation of the target, i.e., non-severe, (denoted ``0'') or the target (denoted ``1''), i.e., severe.
The right-hand-side tree in Figure~\ref{fig:fft1} is 01110 since:
\bi
\item
The first level found a rule that exits to the negation of the target: hence, ``0''.
\item
While the next tree levels found rules that
exit first to target; hence, ``111''.
\item
And the final line of the model exits
to the opposite of the penultimate line; hence, the final ``0''.
\ei

\begin{figure}[!b]

 {\normalsize
\begin{minipage}{\linewidth}
\begin{tabular}{p{\linewidth}}
  \qquad  if  \qquad \ \ topic 1 $>$  0.80   \qquad \qquad  then false \qquad \#0 \\
  \qquad  else if \ \ topic 7 $>$  0.60    \qquad \qquad   then true \qquad \  \#1 \\
   \qquad else if \ \ topic 3 $>$  0.65   \qquad \qquad  then true \qquad \  \#1 \\
   \qquad else if \ \ topic 5 $\leq$ 0.50    \qquad \qquad  then true \qquad \  \#1 \\
   \qquad else \qquad \qquad \qquad \qquad \qquad \qquad false \qquad \qquad \ \#0 
\end{tabular}
\end{minipage}
}
\vspace{-0.25cm}
\caption{Example of an FFT}
\label{fig:fft1}
\end{figure}

Following the advice of~\cite{fu2018building,chen2018applications,phillips2017fftrees}, for all the experiments of this paper, we use a depth    $d=4$. 
For trees of depth $d=4$, there are $2^4=16$ possible trees which can be denoted as 00001, 00010, 00101, ... , 11110. During FFT training, all $2^d$ trees are generated, then we select the best one (using the training data).
 This single best tree is then applied to the test data.
 Note that FFTs of such small
depths are very succinct
(see examples in Figures~\ref{fig:fft} and~\ref{fig:fft1}). Such FFTs generate rules which leads to decision of finding a report as severe and non-severe for the datasets under study. Many other data mining algorithms used in software analytics are far less
succinct and far less comprehensible as explained in \tion{comprehensibility}.


% \begin{table}[!t]
% \caption{Comprehension issues with models generated by data mining algorithms used in software analytics.}\label{tab:bad}
% \begin{tabular}{|p{.99\linewidth}|}\hline
% \rowcolor{gray!20}
% For very high dimensional data, there is some evidence that complex deep learning algorithms
%     have  advantages for software engineering applications~\cite{yang2015deep, white2015toward,gu2016deep}. 
%     However, since they do not readily support   explainability, they have been  criticizing   as 
%     ``data mining alchemy''~\cite{DL2017alchemy}. \\

% Support vector machines and principle component methods achieve their results after synthesizing new dimensions
% which are totally unfamiliar to human users~\cite{Menzies2009ExplanationVP}.\\\rowcolor{gray!20}

% Other methods that are heavily based on mathematics can be hard to explain to most
% users. For example, in our experience, it is hard for (e.g.,) users to determine minimal changes to a project that mostly affect defect-proneness, just by browsing the internal frequency tables of a Naive Bayes classifier or the coefficients found via linear regression/logistic regression~\cite{Menzies2009ExplanationVP}.\\

% When decision tree learners  are many pages long, they are  hard to browse and understand~\cite{friedl1997decision}.\\\rowcolor{gray!20}

% Random forests are even harder to understand than decision trees since the problems of reading one tree are multiplied $N$ times, one for each member of the forest~\cite{liaw2002classification}. \\

% Instance-based methods do not compress their training data; instead they produce  conclusions by  finding older exemplars closest to the new example. Hence, for such instance-based methods, it is hard to generalize and make a conclusion about what kind of future projects might be (e.g.,) most defective-prone~\cite{aha1991instance}.
% \\\hline
% \end{tabular}
% \end{table}  


%  The value of models such as FFTs comprising many small rules has been extensively studied:
%  \bi
%  \item
%              These    models use very few
%              attributes from the data. Hence they tend to be robust against overfitting, especially on small and noisy data, and have been found to predict data at levels comparable with regression. See for example~\cite{martignon2008categorization, woike2017integrating,czerlinski1999good}.
%         \item
%         Other   work has  shown that these rule-based
%         models can perform comparably well to more  complex models in a range of domains
%         e.g., public health, medical risk management, performance science, etc.~\cite{jenny2013simple, laskey2014comparing, raab2015power}.
%         \item
%           Neth and Gigerenzer
%           argue that such rule-bases  are tools that   work well under conditions of uncertainty~\cite{neth2015heuristics}. 
%           \item
%       Brighton showed that rule-based models can perform better than 
%         complex nonlinear algorithms such as  neural networks, 
%         exemplar models, and classification/regression trees~\cite{brighton2006robust}.
%  \ei      